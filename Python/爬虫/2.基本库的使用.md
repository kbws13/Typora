# urllib的使用

使用urllib库可以实现HTTP请求的发送，而且不需要关心HTTP协议本身甚至更底层的实现，我们要做的就是指定请求的URL、请求头、请求体等信息。

urllib是python内置的HTTP请求库，包含如下四个模块：

- request：这是最基本的HTTP请求模块，可以模拟请求的发送
- error：异常处理模块。如果出现请求异常，那么我们可以捕获这些异常然后进行重试或其他操作以保证程序运行不会意外终止
- parse：一个工具模块。提供了许多URL的处理方法，如拆分、解析、合并等
- robotparser：主要用来识别网站的robots.txt文件，然后判断哪些网站可以爬，哪些网站不可以

## 发送请求

使用urllib的request模块，可以方便的发送请求并得到响应

- urlopen

    `urllib.request`模块提供了最基本的构造HTTP请求的方法，利用这个模块可以模拟浏览器的请求发起过程，同时它还有处理授权验证、重定向、浏览器Cookie等

    例如，抓取python官网

    ```python
    import urllib.request
    
    response = urllib.request.urlopen('https://www.python.org')
    print(reponse.read().decode('utf-8'))
    ```

    如果想看返回的响应是什么，使用type方法输出响应类型，调用read方法可以得到响应的网页内容、调用status属性可以得到响应结果的状态码

    ```python
    import urllib.request
    
    response = urllib.request.urlopen('https://www.python.org')
    print(type(response))
    //输出结果为<class 'http.client.HTTPResponse'>
    //响应是一个HTTPResponse类型的对象，主要包含read、readinto、getheader、getheaders、fileno等方法，以及msg、version、status、reason、debuglevel、closed等属性
    
    print(response.status)
    print(response.getheaders())
    print(response.getheader('Server'))
    //前两个输出分别是响应的状态码和响应的头信息；最后一个输出是调用getheader方法，并传入参数Server，获取了响应头中Server的值，结果是nginx，意思是服务器是用Nginx搭建的
    ```

    urlopen方法的API：

    `urllib.request.urlopen(url,data=None,[timeout,]*,cafile=None,capth=None,cadefault=false,context=None)`

- data参数

    data参数是可选的。在添加该参数的时候，需要使用bytes方法将参数转化为字节流编码格式的内容，即bytes类型。另外，如果传递了这个参数，那么它的请求方式就不是GET了，而是POST。例如

    ```python
    import urllib.parse
    import urllib.request
    
    data = bytes(urllib.parse.urlencode({'name':'germey'}),encoding='utf-8')
    response = urllib.request.urlopen('https://www.httpbin.org/post',data=data)
    print(response.read().decode('utf-8'))
    ```

    传递的参数是name，值是germey，用bytes方法将它转码成bytes类型。该方法的第一个参数是str类型，因此用`urllib.parse`模块里的urlencode方法将字典参数转化为字符串，第二个参数用于指定编码格式，这里指定为utf-8

- timeout参数

    用于设置超时时间，单位为秒，意思是如果请求超过了设置的时间，还没有得到响应就会抛出异常。如果不指定该参数，就会使用全局默认时间。这个参数支持HTTP、FTP请求

    ```python
    import urllib.request
    
    response = urllib.request.urlopen('https://www.httpbin.org/get',timeout=0.1)
    print(response.read())
    //结果可能会报错，因为我们设置超时时间为0.1秒，超过这个世界依然没有响应就会抛出URLError异常。该异常属于urllib.error模块，错误原因是超时
    ```

    利用try expect语句

    ```python
    import urllib.request
    
    try:
    	response = urllib.request.urlopen('https://www.httpbin.org/get',timeout=0.1)
    expect urllib.error.URLError as e:
        if isinstance(e.reason,socket.timeout):
    		print('TIME OUT')
    ```

- 其他参数

    除了data参数和timeout参数，urlopen方法还有context参数，该参数必须是`ssl.SSLContext`类型，用来指定SSL的设置

    此外，cafile和capath这两个参数分别用来指定CA证书和其路径，这两个在请求HTTPS链接时会有用

    cadefault参数现在已经弃用了，默认为false

- Request

    使用urlopen方法可以发送最基本的请求，但是不足以构成一个完整的请求。如果需要添加Headers等信息，就得利用更强大的Request类来构建请求。例如：

    ```python
    import urllib.request
    
    request = urllib.request.Request('https://python.org')
    response = urllib.request.urlopen(request)
    print(response.read().decode('utf-8'))
    ```

    这次方法的参数不是url，而是一个Request类型的对象。通过构造这个数据结构，一方面可以单独成立一个对象，另一方面可以更加灵活的配置参数

    Request类的构造方法如下：

    `class urllib.request.Request(url,data=None,headers={},origin_req_host=None,unverifiable=False`

    `,Method=None)`

    第一个参数url用于请求URL，这是必传参数，其他都是可选参数

    第二个参数data如果要传数据，必须要bytes类型的。如果数据是字典，可以先用`urllib.parse`模块里的urlencode方法进行编码

    第三个参数headers是一个字典，这就是请求头，我们在构造请求的时候，既可以通过headers参数直接构造此项，还可以通过调用请求示例的`add_header`方法添加。

    添加请求头最常见的方法是通过修改User-Agent来伪装浏览器。默认的User-Agent是Python-urllib，我们可以通过修改这个值来伪装浏览器。例如要伪装火狐浏览器，就可以把User-Agent设置为：

    `Mozilla/5.0 (X11;U;Linux i686) Gecko/20071127 Firefox/2.0.0.11`

    第四个参数`origin_req_host`指的是请求方的host名称或者IP地址

    第五个参数unverifiable表示请求是否无法验证，默认取值为false，意思是用户没有足够权限来接受这个请求的结果。例如，一个请求HTML文档中的图片，但是没有自动抓取图像的权限，这时的unverifiable的值就是True

    第六个参数method是一个字符串，用来指示请求使用的方法，例如GET、POST和PUT等

    多参数构造Request类

    ```python
    from urllib import request,parse//导入相关模块
    
    url = 'https://www.httpbim.org/post' //设置URL
    headers = {	//设置请求头
        'User-Agent':'Mozilla/4.0 (cpmpatible; MSIE 5.5;Windows NT)',
        'Host':'www.httpbin.org'
    }
    dict = {'name':'germey'}
    data = bytes(parse.urlencode(dict),encoding='utf-8')//将数据转为bytes类型字节流格式
    req = request.Response(url=url,data=data,headers=headers,method='POST')//指定请求方法为POST
    response = request.urlopen(req)
    print(response.read().decode('utf-8'))
    ```

- 高级用法

    对于Cookie处理、代理设置等需要使用Handler，Handler可以理解为各种处理器，有处理登录验证的、有处理Cookie的，有处理代理设置的。利用这些功能，几乎可以实现HTTP请求中的所有功能

    首先介绍`urllib.request`模块里的BaseHandler类，这是所有其他Handler类的父类，提供了最基础的方法

    - HTTPDefaultErrorHandler用于处理HTTP响应错误，所有错误都会抛出HTTPError异常
    - HTTPRedirectHandler用于处理重定向
    - HTTPCookieProcessor用于处理Cookie
    - HTTPPasswordMgr用于管理密码，它维护着用户名密码的对照表
    - HTTPBasicAuthHandler用于管理认证

- 验证

    在访问某些网站的时候，例如https://ssr3.scrape.center，可能会弹出认证窗口。

    遇到这种请求，是这个网站开启了基本的身份认证，这是一种登录验证方式。爬虫可以使用`HTTPBasicAuthHandler`模块进行请求页面

    ```python
    from urlib.request import HTTPPasswordMgriWithDefaultRealm,HTTPBasicAuthHandler,build_opener
    from urllib.error import URLERROR
    
    username = 'admin'
    password = 'admin'
    url = 'https://ssr3.scrape.center/'
    
    p = HTTPPasswordMgriWithDefaultRealm()
    # 添加用户名和密码
    p.add_password(None,url,username,password)
    # 实例化HTTPBasicAuthHandler，建立了一个用来处理验证的Handler类
    auth_handler = HTTPBasicAuthHandler(p)
    # 建立了一个Opener，这个Opener在发送请求时就相当于验证成功了
    opener = build_opener(auth_handler)
    
    try:
        # 使用open方法打开链接
        result = opener.open(url)
        html = result.read().decode('utf-8')
        print(html)
    except URLERROR as e:
        print(e.reason)
    ```

- 代理

    如果要添加代理，需要在本地搭建一个HTTP代理，并让其运行在8080端口上

    使用ProxyHandler，其参数是一个字典，键名是协议类型，键值是代理链接，可以添加多个代理

    ```python
    from urllib.error import URLERROR
    from urllib.request import ProxyHandler,build_opener
    
    # 使用Handler和build_opener方法构建了一个Opener
    proxy_handler = ProxyHandler({
        'http':'http://127.0.0.1:8080',
        'https':'https://127.0.0.1:8080'
    })
    opener = build_opener(proxy_handler)
    # 发送请求
    try:
        response = opener.open('https://www.baidu.com')
        print(response.read.decode('utf-8'))
    except URLERROR as e:
        print(e.reason)
    ```

- Cookie

    处理Cookie需要用相关的Handler

    获取网站的Cookie

    ```python
    import http.cookiejar,urllib.request
    
    # 声明一个CookieJar对象
    cookie = http.cookiejar.CookieJar()
    # 利用HTTPCookieProcessor构建一个Handler
    handler = urllib.request.HTTPCookieProcessor(cookie)
    # 利用build_opener方法构建Opener
    opener = urllib.request.build_opener(handler)
    # 构建open函数
    response = opener.open('https://www.baidu.com')
    for item in cookie:
        print(item.name + "=" + item.value)
    # 结果分别输出了每条Cookie条目的名称和值
    ```

    将Cookie以文本形式保存

    ```python
    import http.cookiejar,urllib.request
    
    filename = 'cookie.txt'
    # MozillaCookieJar是CookieJar的子类，用来处理跟Cookie和文件相关的事件，例如读取和保存Cookie，可以将Cookie保存成Mozilla型浏览器的Cookie格式
    cookie = http.cookiejar.MozillaCookieJar(filename)
    handler = urllib.request.HTTPCookieProcess(cookie)
    opener = urllib.request.build_opener(handler)
    response = opener.open('https://www.baidu.com')
    cookie.save(ignore_discard=True,ignore_expires=True)
    ```

    保存成LWP格式的Cookie文件，只需要修改为

    `cookie = http.cookiejar.LWPCookieJar(filename)`

    生成Cookie文件后进行读取

    ```python
    import http.cookiejar,urllib.request
    
    cookie = http.cookiejar.LWPCookieJar()
    # 调用load方法读取本地的Cookie文件，获取了Cookie内容
    cookie.load('cookie.txt',ignore_dicard=True,ignore_expires=True)
    handler = urllib.request.HTTPCookieProcess(cookie)
    opener = urllib.request.build_opener(handler)
    response = opener.open('https://www.baidu.com')
    # 运行结果正确会输出百度的源代码
    print(response.read().decode('utf-8'))
    ```

## 处理异常

- URLError

    URLError是error异常类的基类，由request模块产生的异常都可以通过捕获这个类来处理

    它有一个reason属性，即返回错误的原因

    ```python
    import urllib import request,error
    
    try:
        response = request.urlopen('https://cuiqingcai.com/404')
    except error.URLERROR as e:
        print(e.reason)
    # 结果会输出错误原因
    ```

- HTTPError

    HTTPError是URLError的子类，专门用来处理HTTP请求错误，例如认证失败等。有如下属性：

    - code：返回HTTP的状态词，例如404，500
    - reason：返回错误的原因
    - headers：返回请求头

    例如：

    ```python
    import urllib import request,error
    
    try:
        response = request.urlopen('https://cuiqingcai.com/404')
    except error.HTTPError as e:
        print(e.reason,e.code,e.headers,sep='\n')
    # 运行结果如下
    Not Found
    404
    Server:nginx/1.10.3(Ubuntu)
    ......
    ```

    因为URLError是HTTPError异常，所有可以先捕获子类异常，在捕获父类的错误

    ```python
    import urllib import request,error
    
    try:
        response = request.urlopen('https://cuiqingcai.com/404')
    except error.HTTPError as e:
        print(e.reason,e.code,e.headers,sep='\n')
    except error.URLERROR as e:
        print(e.reason)
    else:
        print('Request Successfully')
    ```

    这是一个比较好的异常处理写法

    有时候，reason属性返回的不一定是字符串，也可能是一个对象

    ```python
    import socket
    import urllib.request
    import urllib.error
    
    try:
        response = urllib.request.urlopen('https://www.baidu.com'.timeout=0.1)
    except urllib.error.URLError as e:
        print(type(e.reason))
        if isinstance(e.reason,socket.timout):
            print('TIME OUT')
    # 运行结果如下
    <class'socket.timeout'>
    TIME OUT
    ```

## 解析链接

urllib库提供了parse模块，这个模块定义了处理URL的标准接口，例如实现URL各部分的抽取、合并以及链接转换。它支持如下协议的URL处理：file、ftp、gopher、hdl、http、https、imap、mailto、mms、news、nntp、prospero、rsync、rtsp、rtspu、sftp、sip、sips、snews、svn、svn+ssh、telnet、wais

- urlparse

    

# requests的使用



# 正则表达式



# httpx的使用



# 基础爬虫实战